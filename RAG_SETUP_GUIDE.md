# RAG System Setup & Integration Guide

## üéØ Overview

This guide will help you integrate the new Hybrid RAG system into your social media post scheduler. The system consists of two main components:

1. **"The Learner" (Ingestion Pipeline)**: Automatically learns from posted content using Gemini Vision
2. **"The Talker" (Chat Pipeline)**: Responds to DMs/comments using Llama 3 + Pinecone retrieval

---

## üìã Prerequisites

### Required API Keys

Add these to your `.env` file:

```env
# Existing keys (keep these)
GEMINI_API_KEY=your_gemini_api_key_here
INSTAGRAM_ACCESS_TOKEN=your_token_here
INSTAGRAM_BUSINESS_ACCOUNT_ID=your_account_id_here

# New RAG System Keys
GROQ_API_KEY=your_groq_api_key_here
PINECONE_API_KEY=your_pinecone_api_key_here
PINECONE_ENVIRONMENT=us-east-1
PINECONE_INDEX_NAME=social-media-posts

# Optional: Override defaults
RAG_RETRIEVAL_K=1
RAG_MAX_CONTEXT_TOKENS=200
RAG_RATE_LIMIT_DELAY=2.0
```

### Get Your API Keys

1. **Groq API** (Free Tier - 30 req/min):
   - Visit: https://console.groq.com/
   - Sign up and create an API key
   - Free tier includes Llama 3 access

2. **Pinecone** (Free Tier - Serverless Starter):
   - Visit: https://www.pinecone.io/
   - Sign up for free account
   - Create a new index with these settings:
     - **Dimension**: 768
     - **Metric**: cosine
     - **Cloud**: AWS
     - **Region**: us-east-1

3. **Gemini API** (You already have this):
   - Your existing key works for both Vision and Embeddings

---

## üöÄ Installation

Install the new dependencies:

```bash
pip install -r requirements.txt
```

Or install manually:

```bash
pip install langchain langchain-community langchain-google-genai langchain-groq langchain-pinecone pinecone-client Pillow
```

---

## üîß Integration Steps

### Step 1: Ingest Existing Posts (One-Time Setup)

Create a migration script to ingest your existing posts:

```python
# migrate_existing_posts.py

from app import db
from app.models import Post
from app.ai.rag_ingest import get_ingestion_pipeline

def migrate_posts():
    """Ingest all existing published posts into RAG system."""
    
    pipeline = get_ingestion_pipeline()
    
    # Get all published posts with images
    posts = Post.query.filter(
        Post.status == 'published',
        Post.image_url.isnot(None)
    ).all()
    
    print(f"Found {len(posts)} posts to ingest...")
    
    success_count = 0
    failed_count = 0
    
    for post in posts:
        print(f"Processing post {post.id}...")
        
        success = pipeline.ingest_post(
            post_id=str(post.id),
            image_url=post.image_url,
            caption=post.caption or "",
            platform=post.platform,
            scheduled_time=post.scheduled_time
        )
        
        if success:
            success_count += 1
        else:
            failed_count += 1
    
    print(f"\nMigration complete!")
    print(f"‚úì Success: {success_count}")
    print(f"‚úó Failed: {failed_count}")

if __name__ == '__main__':
    migrate_posts()
```

Run it:

```bash
python migrate_existing_posts.py
```

### Step 2: Auto-Ingest New Posts

Update your post scheduling logic to automatically ingest new posts.

**Option A: In `routes.py` (when post is created/scheduled)**

```python
from app.ai.rag_ingest import ingest_scheduled_post

@app.route('/api/schedule_post', methods=['POST'])
def schedule_post():
    # ... your existing post creation code ...
    
    new_post = Post(
        caption=caption,
        image_url=image_url,
        platform=platform,
        scheduled_time=scheduled_time,
        status='scheduled'
    )
    db.session.add(new_post)
    db.session.commit()
    
    # NEW: Automatically ingest into RAG system
    try:
        ingest_scheduled_post(
            post_id=str(new_post.id),
            image_url=new_post.image_url,
            caption=new_post.caption,
            platform=new_post.platform,
            scheduled_time=new_post.scheduled_time
        )
        logger.info(f"Post {new_post.id} ingested into RAG system")
    except Exception as e:
        logger.error(f"Failed to ingest post {new_post.id}: {e}")
        # Don't fail the post creation if ingestion fails
    
    return jsonify({'success': True, 'post_id': new_post.id})
```

**Option B: In your posting function (when post goes live)**

```python
from app.ai.rag_ingest import ingest_scheduled_post

def publish_post_to_instagram(post):
    """Publish post and ingest into RAG system."""
    
    # ... your existing Instagram API code ...
    
    # After successful posting
    if post_successful:
        post.status = 'published'
        db.session.commit()
        
        # NEW: Ingest into RAG system
        try:
            ingest_scheduled_post(
                post_id=str(post.id),
                image_url=post.image_url,
                caption=post.caption,
                platform='instagram'
            )
        except Exception as e:
            logger.error(f"RAG ingestion failed: {e}")
```

### Step 3: Auto-Reply to DMs

Update your DM webhook handler to use the RAG system.

**In `app/dm_routes.py` or `app/social/instagram_dm_sync.py`:**

```python
from app.ai.rag_chat import generate_dm_response

@app.route('/webhook/instagram', methods=['POST'])
def instagram_webhook():
    """Handle Instagram webhook events."""
    
    data = request.json
    
    # Check if it's a message event
    if 'entry' in data:
        for entry in data['entry']:
            for messaging_event in entry.get('messaging', []):
                
                # Handle incoming message
                if 'message' in messaging_event:
                    sender_id = messaging_event['sender']['id']
                    message_text = messaging_event['message'].get('text', '')
                    
                    if message_text:
                        # NEW: Use RAG system to generate response
                        try:
                            response = generate_dm_response(
                                message=message_text,
                                conversation_id=sender_id
                            )
                            
                            # Send response via Instagram API
                            send_instagram_dm(sender_id, response)
                            
                            logger.info(f"Auto-replied to {sender_id}: {response}")
                            
                        except Exception as e:
                            logger.error(f"Failed to generate/send response: {e}")
    
    return jsonify({'status': 'ok'})


def send_instagram_dm(recipient_id: str, message: str):
    """Send a DM via Instagram API."""
    url = f"https://graph.facebook.com/v18.0/me/messages"
    
    payload = {
        'recipient': {'id': recipient_id},
        'message': {'text': message},
        'access_token': Config.INSTAGRAM_ACCESS_TOKEN
    }
    
    response = requests.post(url, json=payload)
    response.raise_for_status()
    return response.json()
```

### Step 4: Auto-Reply to Comments (Optional)

If you want to auto-reply to comments as well:

```python
from app.ai.rag_chat import generate_dm_response

def handle_instagram_comment(comment_data):
    """Handle Instagram comment and auto-reply."""
    
    comment_id = comment_data['id']
    comment_text = comment_data['text']
    commenter_username = comment_data['username']
    
    # Generate response using RAG
    response = generate_dm_response(
        message=comment_text,
        conversation_id=f"comment_{comment_id}"
    )
    
    # Reply to comment via Instagram API
    reply_to_comment(comment_id, response)
    
    logger.info(f"Replied to @{commenter_username}: {response}")


def reply_to_comment(comment_id: str, reply_text: str):
    """Reply to an Instagram comment."""
    url = f"https://graph.facebook.com/v18.0/{comment_id}/replies"
    
    payload = {
        'message': reply_text,
        'access_token': Config.INSTAGRAM_ACCESS_TOKEN
    }
    
    response = requests.post(url, params=payload)
    response.raise_for_status()
    return response.json()
```

---

## üß™ Testing the System

### Test the Ingestion Pipeline

```python
# test_rag_ingest.py

from app.ai.rag_ingest import get_ingestion_pipeline

pipeline = get_ingestion_pipeline()

# Test with a sample post
success = pipeline.ingest_post(
    post_id="test_001",
    image_url="https://example.com/event-image.jpg",
    caption="Join us for the Annual Tech Summit 2025 at Convention Center on March 15th!",
    platform="instagram"
)

print(f"Ingestion {'successful' if success else 'failed'}!")
```

### Test the Chat Pipeline

```python
# test_rag_chat.py

from app.ai.rag_chat import test_rag_system

# Run the built-in test suite
results = test_rag_system()

# Or test with custom queries
custom_queries = [
    "Hi there!",  # Should hit gatekeeper
    "When is the next event?",  # Should use RAG
    "Tell me about the venue",  # Should use RAG
]

results = test_rag_system(custom_queries)
```

Run the test:

```bash
python test_rag_chat.py
```

Expected output:
```
============================================================
RAG SYSTEM TEST
============================================================

[Query 1] Hi there!
------------------------------------------------------------
[Response] Hey! Thanks for reaching out! üòä
[Source] gatekeeper
[Tokens] 0
[Time] 5ms

[Query 2] When is the next event?
------------------------------------------------------------
[Response] Based on our recent posts, the next event is on March 15th at the Convention Center!
[Source] rag_llm
[Tokens] estimated_50-200
[Time] 2341ms

============================================================
STATISTICS
============================================================
Total Queries: 3
Gatekeeper Hits (0 tokens): 1
RAG Hits (50-200 tokens each): 2
Errors: 0
Token Efficiency: 33.3% queries used 0 tokens
============================================================
```

---

## üìä Token Optimization Breakdown

### Why This Architecture Saves Tokens

1. **Gatekeeper Filter**: 60-80% of messages are greetings
   - Traditional: 100 tokens per greeting
   - Our system: **0 tokens** (static responses)
   - **Savings**: 60-80 tokens per greeting message

2. **k=1 Retrieval**: Only fetch most relevant context
   - Traditional: k=3-5 chunks = 500-1000 tokens
   - Our system: k=1 chunk = **150-200 tokens**
   - **Savings**: 300-800 tokens per query

3. **JSON Compression**: Minimal fact extraction
   - Traditional: Full image descriptions = 300-500 tokens
   - Our system: Compressed JSON = **50-100 tokens**
   - **Savings**: 250-400 tokens per ingestion

4. **Response Length Limit**: max_tokens=150
   - Traditional: Unlimited = 300-500 tokens
   - Our system: **50-150 tokens**
   - **Savings**: 150-350 tokens per response

5. **Conversation Memory**: 200 token limit
   - Traditional: Full history = 1000+ tokens
   - Our system: **Summarized to 200 tokens**
   - **Savings**: 800+ tokens per conversation

### Monthly Token Estimate (100 DMs/day)

**Without Optimization:**
- 100 DMs √ó 30 days = 3,000 DMs
- Average 400 tokens per DM = 1,200,000 tokens/month
- **Cost**: ~$24/month (at $0.02/1K tokens)

**With Our Optimization:**
- 70% hit gatekeeper = 2,100 DMs √ó 0 tokens = 0
- 30% use RAG = 900 DMs √ó 100 tokens = 90,000 tokens
- **Cost**: ~$1.80/month
- **Savings**: 92.5% reduction! üéâ

---

## üö® Rate Limiting & Error Handling

### Groq Free Tier Limits

- **30 requests per minute**
- **14,400 requests per day**

Our system handles this automatically with:
- 2-second delay between API calls (30 req/min max)
- Gatekeeper reduces API usage by 60-80%
- Built-in error handling with fallback responses

### Handling Peak Traffic

If you expect >14,400 DMs/day:

1. **Increase gatekeeper coverage**:
   ```python
   # Add more greeting patterns in rag_chat.py
   GREETING_PATTERNS.extend([
       r'^what.*up$',
       r'^how.*you$',
       # Add more patterns
   ])
   ```

2. **Implement response caching**:
   ```python
   # Cache frequently asked questions
   FAQ_CACHE = {
       "when is the next event": "Our next event is on March 15th!",
       "where is the venue": "We're at Convention Center downtown.",
   }
   ```

3. **Upgrade to Groq Pro** ($0.27/1M tokens - still very cheap!)

---

## üîê Security Best Practices

1. **Never commit `.env` file** to version control
2. **Use Railway/Heroku secrets** for production API keys
3. **Validate webhook signatures** (already in your code)
4. **Implement rate limiting** on your webhook endpoints
5. **Monitor token usage** via logging

---

## üìà Monitoring & Debugging

### Check Pinecone Index Stats

```python
from pinecone import Pinecone

pc = Pinecone(api_key=Config.PINECONE_API_KEY)
index = pc.Index(Config.PINECONE_INDEX_NAME)

stats = index.describe_index_stats()
print(f"Total vectors: {stats['total_vector_count']}")
print(f"Dimension: {stats['dimension']}")
```

### View Recent Responses

Add logging to track all responses:

```python
import logging

logging.basicConfig(
    filename='rag_responses.log',
    level=logging.INFO,
    format='%(asctime)s - %(message)s'
)

# In your DM handler
logger.info(f"User: {message_text}")
logger.info(f"Bot: {response}")
logger.info(f"Source: {metadata['source']}")
logger.info(f"Tokens: {metadata['tokens_used']}")
```

### Test Specific Queries

```python
from app.ai.rag_chat import get_chat_pipeline

pipeline = get_chat_pipeline()

# Test a specific query
response, metadata = pipeline.generate_response("When is the next workshop?")
print(f"Response: {response}")
print(f"Metadata: {metadata}")
```

---

## üéì Next Steps

1. **Deploy to Railway/Heroku** with new environment variables
2. **Run migration script** to ingest existing posts
3. **Enable auto-reply** in your webhook handlers
4. **Monitor token usage** for first week
5. **Tune gatekeeper patterns** based on your DMs
6. **Add more FAQ caching** for common questions

---

## üÜò Troubleshooting

### "Pinecone index does not exist"

Run the ingestion pipeline once to create the index:

```python
from app.ai.rag_ingest import get_ingestion_pipeline
pipeline = get_ingestion_pipeline()
# Index will be created automatically
```

### "Rate limit exceeded"

Increase the delay:

```env
RAG_RATE_LIMIT_DELAY=3.0  # 3 seconds = 20 req/min
```

### "No relevant context found"

Your index might be empty. Check:

```python
from app.ai.rag_chat import get_chat_pipeline
pipeline = get_chat_pipeline()

# Try a test retrieval
results = pipeline.vector_store.similarity_search("event", k=1)
print(f"Found {len(results)} documents")
```

### "Token limit exceeded"

Reduce memory buffer:

```env
RAG_MAX_CONTEXT_TOKENS=100  # Reduce from 200
```

---

## üìö Additional Resources

- **LangChain Docs**: https://python.langchain.com/docs/get_started/introduction
- **Groq API Docs**: https://console.groq.com/docs/quickstart
- **Pinecone Docs**: https://docs.pinecone.io/docs/quickstart
- **Gemini Vision Guide**: https://ai.google.dev/tutorials/python_quickstart

---

## üí° Pro Tips

1. **Start small**: Test with 10-20 posts before ingesting everything
2. **Monitor costs**: Even free tiers have limits - track your usage
3. **Iterate on prompts**: The system prompt in rag_chat.py can be tuned
4. **Use logging**: Add detailed logging to understand user patterns
5. **A/B test**: Try different k values (1, 2, 3) to find optimal retrieval

---

**Questions?** Check the code comments - they explain every optimization decision!

Happy automating! üöÄ
